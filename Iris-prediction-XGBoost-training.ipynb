# Prediction using XGBoost

import numpy as np
import xgboost as xgb
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.datasets import dump_svmlight_file
from sklearn.externals import joblib
from sklearn.metrics import precision_score

dataset = datasets.load_iris()
X = dataset.data
y = dataset.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# use DMatrix for xgbosot
train_data = xgb.DMatrix(X_train, label=y_train)
test_data = xgb.DMatrix(X_test, label=y_test)

# xgboost config
config = {
    'max_depth': 3,  # the maximum depth of each tree
    'eta': 0.1,  # learning rate 
    'verbose': 3,  # verbose
    'objective': 'multi:softprob',  # multiclass classification using the softmax objective
    'num_class': 3  # the number of classes that exist in this datset
}
boost_rounds = 30  # the number of rounds for boosting

# training and testing - numpy matrices
booster = xgb.train(config, train_data, boost_rounds)

booster.dump_model("xgb_predict_train_mdl.txt")

predictions = booster.predict(test_data)
print("Prediction : ", predictions)

# extracting most confident predictions
confident_preds = np.asarray([np.argmax(line) for line in predictions])
print ("Precision score of test data : ", precision_score(y_test, confident_preds, average="micro"))

# save the models for later
joblib.dump(bst, 'xgbooster_predict_train.pkl', compress=True)

print("Model text dump")
!less xgb_predict_train_mdl.txt
